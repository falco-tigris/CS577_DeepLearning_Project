{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "Here, we propose a combination of the *Google ViT* for the image feature extraction (i.e. encoder) and the *Bert* for the text generation (i.e. decoder).\n",
    "\n",
    "We use a chekpoint from [HuggingFace](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder) since we do not have enough resources for training such huge models from scratch (either have enough data for doing so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpizarro/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/mpizarro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data.preprocessing as pr\n",
    "from torchvision import transforms\n",
    "from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n",
    "\n",
    "# Get the data\n",
    "uids = np.unique(pr.projections.index)[:300]\n",
    "\n",
    "# Image preprocessing \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias=False)\n",
    "])\n",
    "\n",
    "train_data, train_loader, val_data, val_loader, test_data, test_loader = pr.create_dataloaders(uids, pr.IMAGES_PATH, batch_size=3, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Removed shared tensor {'decoder.cls.predictions.decoder.bias', 'decoder.cls.predictions.decoder.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training_Loss: 16.109341918894685 || Validation_Loss: 15.309488630294807\n",
      "Epoch 1 Training_Loss: 14.706854710536717 || Validation_Loss: 15.25134525895119\n",
      "Epoch 2 Training_Loss: 14.557857798264099 || Validation_Loss: 15.252157509326938\n",
      "Epoch 3 Training_Loss: 14.464490270192634 || Validation_Loss: 15.282632225751877\n",
      "Epoch 4 Training_Loss: 14.410946164510941 || Validation_Loss: 15.322292834520336\n",
      "Epoch 5 Training_Loss: 14.36470880550621 || Validation_Loss: 15.344239586591717\n",
      "Epoch 6 Training_Loss: 14.348493299653052 || Validation_Loss: 15.397416532039639\n",
      "Epoch 7 Training_Loss: 14.301939221610011 || Validation_Loss: 15.43715462684631\n",
      "Epoch 8 Training_Loss: 14.278168370238454 || Validation_Loss: 15.414231383800505\n",
      "Epoch 9 Training_Loss: 14.261726054470097 || Validation_Loss: 15.460633134841917\n",
      "Epoch 10 Training_Loss: 14.241429316259072 || Validation_Loss: 15.52247618436813\n",
      "Epoch 11 Training_Loss: 14.631924027890221 || Validation_Loss: 15.528285914659497\n",
      "Epoch 12 Training_Loss: 14.220341214036521 || Validation_Loss: 15.580065286159517\n",
      "Epoch 13 Training_Loss: 14.198633596960422 || Validation_Loss: 15.597272253036502\n",
      "Epoch 14 Training_Loss: 14.234477661352239 || Validation_Loss: 15.662200617790221\n",
      "Epoch 15 Training_Loss: 14.184174562977486 || Validation_Loss: 15.526403832435612\n",
      "Epoch 16 Training_Loss: 14.171459932242879 || Validation_Loss: 15.547195404767995\n",
      "Epoch 17 Training_Loss: 14.158527228684553 || Validation_Loss: 15.555544292926793\n",
      "Epoch 18 Training_Loss: 14.148194032432766 || Validation_Loss: 15.569295352697369\n",
      "Epoch 19 Training_Loss: 14.151102690570113 || Validation_Loss: 15.57901591062546\n",
      "Epoch 20 Training_Loss: 14.145605692821254 || Validation_Loss: 15.585667312145235\n",
      "Epoch 21 Training_Loss: 14.16332133259393 || Validation_Loss: 15.605190491676328\n",
      "Epoch 22 Training_Loss: 14.12908239280227 || Validation_Loss: 15.60932448506355\n",
      "Epoch 23 Training_Loss: 14.126838698851326 || Validation_Loss: 15.637155097723001\n",
      "Epoch 24 Training_Loss: 14.12359795739164 || Validation_Loss: 15.66612347364426\n",
      "Epoch 25 Training_Loss: 14.116882678681769 || Validation_Loss: 15.694414705038069\n",
      "Epoch 26 Training_Loss: 14.108292311693718 || Validation_Loss: 15.72315990328789\n",
      "Epoch 27 Training_Loss: 14.075587591238776 || Validation_Loss: 15.767680597305297\n",
      "Epoch 28 Training_Loss: 13.935161803675959 || Validation_Loss: 15.969451802968976\n",
      "Epoch 29 Training_Loss: 13.820812938487643 || Validation_Loss: 15.899061268568044\n",
      "Epoch 30 Training_Loss: 13.761740032550515 || Validation_Loss: 16.076865047216415\n",
      "Epoch 31 Training_Loss: 13.806943188726377 || Validation_Loss: 16.538454973697664\n",
      "Epoch 32 Training_Loss: 13.758207266309624 || Validation_Loss: 15.954359990358354\n",
      "Epoch 33 Training_Loss: 13.749061135064188 || Validation_Loss: 16.58980695605278\n",
      "Epoch 34 Training_Loss: 13.77404088678614 || Validation_Loss: 16.052497196197514\n",
      "Epoch 35 Training_Loss: 13.78605623371833 || Validation_Loss: 16.139777374267577\n",
      "Epoch 36 Training_Loss: 13.780475240893084 || Validation_Loss: 16.691642481088632\n",
      "Epoch 37 Training_Loss: 13.772901385231355 || Validation_Loss: 16.187283289432532\n",
      "Epoch 38 Training_Loss: 13.746604301233212 || Validation_Loss: 16.02047169208527\n",
      "Epoch 39 Training_Loss: 13.792729299680317 || Validation_Loss: 16.41247722506523\n",
      "Epoch 40 Training_Loss: 13.78287673840479 || Validation_Loss: 17.007082164287567\n",
      "Epoch 41 Training_Loss: 13.731677215711198 || Validation_Loss: 16.716581428051\n",
      "Epoch 42 Training_Loss: 13.775516611284916 || Validation_Loss: 16.23683649897575\n",
      "Epoch 43 Training_Loss: 13.764016915211636 || Validation_Loss: 16.33000030517578\n",
      "Epoch 44 Training_Loss: 13.749973885780943 || Validation_Loss: 16.8402427971363\n",
      "Epoch 45 Training_Loss: 13.786448982964584 || Validation_Loss: 15.77666770815849\n",
      "Epoch 46 Training_Loss: 14.182228542007184 || Validation_Loss: 15.674240452051167\n",
      "Epoch 47 Training_Loss: 14.197227425279863 || Validation_Loss: 15.678323835134508\n",
      "Epoch 48 Training_Loss: 14.192437973697626 || Validation_Loss: 15.696317130327227\n",
      "Epoch 49 Training_Loss: 14.19177099877754 || Validation_Loss: 15.666678529977792\n",
      "Epoch 50 Training_Loss: 14.188337157257896 || Validation_Loss: 15.663766723871234\n",
      "Epoch 51 Training_Loss: 14.19122667861196 || Validation_Loss: 15.678025954961772\n",
      "Epoch 52 Training_Loss: 14.191537181888007 || Validation_Loss: 15.683320015668873\n",
      "Epoch 53 Training_Loss: 14.188842503370452 || Validation_Loss: 15.68822757005692\n",
      "Epoch 54 Training_Loss: 14.18956093872542 || Validation_Loss: 15.68222130537033\n",
      "Epoch 55 Training_Loss: 14.187832916732381 || Validation_Loss: 15.694630950689326\n",
      "Epoch 56 Training_Loss: 14.18488840719241 || Validation_Loss: 15.679996871948243\n",
      "Epoch 57 Training_Loss: 14.175502057624069 || Validation_Loss: 15.70753787755966\n",
      "Epoch 58 Training_Loss: 14.19091234375944 || Validation_Loss: 15.68163082003594\n",
      "Epoch 59 Training_Loss: 14.155291042496671 || Validation_Loss: 15.739741134643559\n",
      "Epoch 60 Training_Loss: 14.127977082159669 || Validation_Loss: 15.744096487760554\n",
      "Epoch 61 Training_Loss: 14.124753783234446 || Validation_Loss: 15.802515751123423\n",
      "Epoch 62 Training_Loss: 14.13793590849481 || Validation_Loss: 15.65073015093803\n",
      "Epoch 63 Training_Loss: 14.127281811385028 || Validation_Loss: 15.755977863073346\n",
      "Epoch 64 Training_Loss: 14.111816340843134 || Validation_Loss: 15.799946653842921\n",
      "Epoch 65 Training_Loss: 14.10044351510241 || Validation_Loss: 15.839495044946666\n",
      "Epoch 66 Training_Loss: 14.087388129360914 || Validation_Loss: 15.94320427179336\n",
      "Epoch 67 Training_Loss: 14.07116796485091 || Validation_Loss: 16.006795048713684\n",
      "Epoch 68 Training_Loss: 14.057534392956082 || Validation_Loss: 16.115312427282333\n",
      "Epoch 69 Training_Loss: 14.02387113908751 || Validation_Loss: 16.317291891574857\n",
      "Epoch 70 Training_Loss: 14.01840623501128 || Validation_Loss: 16.217047017812735\n",
      "Epoch 71 Training_Loss: 14.041060627034284 || Validation_Loss: 16.09014416337013\n",
      "Epoch 72 Training_Loss: 14.028530139838706 || Validation_Loss: 16.192002123594285\n",
      "Epoch 73 Training_Loss: 14.022112759868655 || Validation_Loss: 16.216927194595343\n",
      "Epoch 74 Training_Loss: 14.031812138262044 || Validation_Loss: 16.155504095554353\n",
      "Epoch 75 Training_Loss: 14.016599754316626 || Validation_Loss: 16.112784671783448\n",
      "Epoch 76 Training_Loss: 13.996811712737628 || Validation_Loss: 16.189414417743677\n",
      "Epoch 77 Training_Loss: 14.057682718850867 || Validation_Loss: 15.700965207815177\n",
      "Epoch 78 Training_Loss: 14.057159130552169 || Validation_Loss: 15.679567646980285\n",
      "Epoch 79 Training_Loss: 14.036613217497292 || Validation_Loss: 15.592465984821326\n",
      "Epoch 80 Training_Loss: 14.100396863127177 || Validation_Loss: 15.610879731178281\n",
      "Epoch 81 Training_Loss: 14.020712827159233 || Validation_Loss: 15.655042713880547\n",
      "Epoch 82 Training_Loss: 14.062876749882655 || Validation_Loss: 15.608877569437025\n",
      "Epoch 83 Training_Loss: 14.076885894336531 || Validation_Loss: 15.706850361824037\n",
      "Epoch 84 Training_Loss: 14.205818699524466 || Validation_Loss: 15.700725311040875\n",
      "Epoch 85 Training_Loss: 14.214330711195966 || Validation_Loss: 15.643546408414835\n",
      "Epoch 86 Training_Loss: 14.243538407097878 || Validation_Loss: 15.599241685867312\n",
      "Epoch 87 Training_Loss: 14.261101707948 || Validation_Loss: 15.616358089447022\n",
      "Epoch 88 Training_Loss: 14.26352035682813 || Validation_Loss: 15.614877188205721\n",
      "Epoch 89 Training_Loss: 14.268279067182961 || Validation_Loss: 15.57786989808083\n",
      "Epoch 90 Training_Loss: 14.257430777085569 || Validation_Loss: 15.56654485464096\n",
      "Epoch 91 Training_Loss: 14.244014216735296 || Validation_Loss: 15.578632020950316\n",
      "Epoch 92 Training_Loss: 14.243178789594532 || Validation_Loss: 15.555069983005518\n",
      "Epoch 93 Training_Loss: 14.239347493754032 || Validation_Loss: 15.556923609972001\n",
      "Epoch 94 Training_Loss: 14.237920588096687 || Validation_Loss: 15.5362874865532\n",
      "Epoch 95 Training_Loss: 14.21641468579789 || Validation_Loss: 15.580807489156715\n",
      "Epoch 96 Training_Loss: 14.206594013534811 || Validation_Loss: 15.600431120395665\n",
      "Epoch 97 Training_Loss: 14.212976719425841 || Validation_Loss: 15.602543216943742\n",
      "Epoch 98 Training_Loss: 14.19364411641012 || Validation_Loss: 15.675794959068302\n",
      "Epoch 99 Training_Loss: 14.201556039067494 || Validation_Loss: 15.629485493898391\n"
     ]
    }
   ],
   "source": [
    "# ViT training\n",
    "# Instance model and optimizer\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=False, do_normalize=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"bert-base-uncased\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "# Hyperparameters\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    t_loss = 0\n",
    "    model.train()\n",
    "    n_epochs_to_stop = 5\n",
    "    for batch in train_data:\n",
    "        imgs, reports = batch[0], batch[1]\n",
    "        pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "        labels, _, att = tokenizer(\n",
    "            reports,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            # vocab_file='./vocab.txt',\n",
    "            return_tensors=\"pt\",\n",
    "        ).values()\n",
    "        loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "        # Some optimizations for training\n",
    "        del pixel_values, labels, att, imgs, reports\n",
    "        # torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch in val_data:\n",
    "        imgs, reports = batch[0], batch[1]\n",
    "        pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "        labels, _, att = tokenizer(\n",
    "            reports,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            # vocab_file='./vocab.txt',\n",
    "            return_tensors=\"pt\",\n",
    "        ).values()\n",
    "        loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "        # Some optimizations for training\n",
    "        del pixel_values, labels, att, imgs, reports\n",
    "        test_loss += loss.item() / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch} Training_Loss: {t_loss} || Validation_Loss: {test_loss}\")\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = t_loss\n",
    "        model.save_pretrained('vit-bert-pretrained')\n",
    "\n",
    "    # Early stopping\n",
    "    if test_loss > best_loss:\n",
    "        n_epochs_to_stop -= 1\n",
    "        if n_epochs_to_stop == 0:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('vit-bert-pretrained_last_epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Training_Loss: 14.201556039067494 || Test_Loss: 8.624162828922273\n"
     ]
    }
   ],
   "source": [
    "# # Load model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained('vit-bert-pretrained')\n",
    "# image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=True, do_normalize=False)\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# model.config.add_cross_attention = True\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for batch in val_loader:\n",
    "    imgs, reports = batch[0], batch[1]\n",
    "    pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "    labels, _, att = tokenizer(\n",
    "        reports,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        # vocab_file='./vocab.txt',\n",
    "        return_tensors=\"pt\",\n",
    "    ).values()\n",
    "    loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "    # Some optimizations for training\n",
    "    del pixel_values, labels, att, imgs, reports\n",
    "    test_loss += loss.item() / len(val_loader)\n",
    "\n",
    "print(f\"Epoch {epoch} Training_Loss: {t_loss} || Test_Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '.', '.', '.', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=True, do_normalize=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"vit-bert-pretrained\")\n",
    "\n",
    "img, report = test_data[0] # This has to be changed\n",
    "pixel_values = image_processor(img, return_tensors=\"pt\").pixel_values\n",
    "labels = tokenizer(report, return_tensors=\"pt\").input_ids\n",
    "logits = model(pixel_values=pixel_values, labels=labels).logits\n",
    "predicted_ids = logits.argmax(-1)\n",
    "tokenizer.convert_ids_to_tokens(predicted_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 4.61k/4.61k [00:00<00:00, 1.85MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 982M/982M [00:31<00:00, 30.8MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 228/228 [00:00<00:00, 863kB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 241/241 [00:00<00:00, 368kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 6.80MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 48.8MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 12.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 120/120 [00:00<00:00, 138kB/s]\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a black and white photo of a person standing in front of a light',\n",
       " 'a black and white photo of a person standing in front of a light',\n",
       " 'a black and white photo of a person standing in front of a light']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(images):\n",
    "  pixel_values = feature_extractor2(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values\n",
    "\n",
    "  output_ids = model2.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer2.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "predict_step([train_data[90][0], train_data[1][0], train_data[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vague increased opacity which appears to be within the left lower lobe. question of this could be developing or resolving pneumonia. lungs are otherwise clear. no pleural effusions or pneumothoraces. heart and mediastinum are stable normal size heart. atherosclerotic vascular disease. degenerative changes in the thoracic spine.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.']\n",
      "['.']\n",
      "['.']\n",
      "['.']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "for i in logits[0]:\n",
    "    idx = i.argmax().item()\n",
    "    print(tokenizer.convert_ids_to_tokens([idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
