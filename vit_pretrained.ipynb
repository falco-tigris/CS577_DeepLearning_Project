{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "Here, we propose a combination of the *Google ViT* for the image feature extraction (i.e. encoder) and the *Bert* for the text generation (i.e. decoder).\n",
    "\n",
    "We use a chekpoint from [HuggingFace](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder) since we do not have enough resources for training such huge models from scratch (either have enough data for doing so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data.preprocessing as pr\n",
    "from torchvision import transforms\n",
    "from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n",
    "\n",
    "# Get the data\n",
    "uids = np.unique(pr.projections.index)[:300]\n",
    "\n",
    "# Image preprocessing \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias=False)\n",
    "])\n",
    "\n",
    "train_data, train_loader, val_data, val_loader, test_data, test_loader = pr.create_dataloaders(uids, pr.IMAGES_PATH, batch_size=6, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT training\n",
    "# Instance model and optimizer\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=False, do_normalize=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"bert-base-uncased\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "# Hyperparameters\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    t_loss = 0\n",
    "    model.train()\n",
    "    n_epochs_to_stop = 5\n",
    "    for batch in train_loader:\n",
    "        imgs, reports = batch[0], batch[1]\n",
    "        pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "        labels, _, att = tokenizer(\n",
    "            reports,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            # vocab_file='./vocab.txt',\n",
    "            return_tensors=\"pt\",\n",
    "        ).values()\n",
    "        loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "        # Some optimizations for training\n",
    "        del pixel_values, labels, att, imgs, reports\n",
    "        # torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch in val_loader:\n",
    "        imgs, reports = batch[0], batch[1]\n",
    "        pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "        labels, _, att = tokenizer(\n",
    "            reports,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            # vocab_file='./vocab.txt',\n",
    "            return_tensors=\"pt\",\n",
    "        ).values()\n",
    "        loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "        # Some optimizations for training\n",
    "        del pixel_values, labels, att, imgs, reports\n",
    "        test_loss += loss.item() / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch} Training_Loss: {t_loss} || Validation_Loss: {test_loss}\")\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = t_loss\n",
    "        model.save_pretrained('vit-bert-pretrained')\n",
    "\n",
    "    # Early stopping\n",
    "    if test_loss > best_loss:\n",
    "        n_epochs_to_stop -= 1\n",
    "        if n_epochs_to_stop == 0:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = VisionEncoderDecoderModel.from_pretrained('vit-bert-pretrained')\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=True, do_normalize=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for batch in val_loader:\n",
    "    imgs, reports = batch[0], batch[1]\n",
    "    pixel_values = image_processor(imgs, return_tensors=\"pt\").pixel_values\n",
    "    labels, _, att = tokenizer(\n",
    "        reports,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        # vocab_file='./vocab.txt',\n",
    "        return_tensors=\"pt\",\n",
    "    ).values()\n",
    "    loss = model(pixel_values=pixel_values, labels=labels, decoder_attention_mask=att).loss\n",
    "    # Some optimizations for training\n",
    "    del pixel_values, labels, att, imgs, reports\n",
    "    test_loss += loss.item() / len(val_loader)\n",
    "\n",
    "print(f\"Epoch {epoch} Training_Loss: {t_loss} || Test_Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", do_rescale=True, do_normalize=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"vit-bert-pretrained\")\n",
    "\n",
    "img, report = test_data[120] # This has to be changed\n",
    "pixel_values = image_processor(img, return_tensors=\"pt\").pixel_values\n",
    "labels = tokenizer(report, return_tensors=\"pt\").input_ids\n",
    "logits = model(pixel_values=pixel_values, labels=labels).logits\n",
    "predicted_ids = logits.argmax(-1)\n",
    "tokenizer.convert_ids_to_tokens(predicted_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
