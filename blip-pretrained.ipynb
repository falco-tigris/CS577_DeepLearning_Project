{"cells":[{"cell_type":"markdown","metadata":{},"source":["#  BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation \n","Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.\n","* Paper: https://arxiv.org/abs/2201.12086"]},{"cell_type":"markdown","metadata":{},"source":["# Radiology Objects in COntext (ROCO): A Multimodal Image Dataset\n","Radiology Objects in COntext (ROCO) dataset, a large-scale medical and multimodal imaging dataset. The listed images are from publications available on the PubMed Central Open Access FTP mirror, which were automatically detected as non-compound and either radiology or non-radiology. Each image is distributed as a download link, together with its caption. Additionally, keywords extracted from the image caption, as well as the corresponding UMLS Semantic Types (SemTypes) and UMLS Concept Unique Identifiers (CUIs) are available. The dataset could be used to build generative models for image captioning, classification models for image categorization and tagging or content-based image retrieval systems.\n","\n","* Dataset: https://github.com/razorx89/roco-dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["# Caption Generation from Chest X-Ray Images:\n","![](https://i.ibb.co/G9bd0bg/chest-Xray.png)"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","Task : Fine tune two ML Models with a custom dataset using the following transformers to generate description of a chest x-ray image.\n","Base Models: The base transformers models to be used for fine tuning :\n","\n","Salesforce/blip-image-captioning-large\n","microsoft/git-large-textcaps\n","\n","Dataset : Radiology Objects in COntext (ROCO): A Multimodal Image Dataset\n","\n","Link : https://www.kaggle.com/datasets/virajbagal/roco-dataset\n","Filter to select only chest x-ray images in the ‘radiology’ folder. You can extract those images and their corresponding captions using search for the following string in the ‘captions’ file: \"chest x-ray\"\n","* Train:\n","    Select ~1800 images\n","* Test and Validation :\n","    Select ~200 images\n","\n","Directions:\n","\n","* Create two training Jupyter Notebooks containing finetune scripts and evaluation results for two captioning models above.\n","* Fine tune and evaluate the model you will save. Explain the steps you followed and show the several predictions to see the quality of the models you trained.\n","* Deploy your model for prediction on a simple web page using Gradio or Streamlit or Fast API or in a container you will build. You will share the urls of your deployed models with us during the second technical interview; so that we will have a chance to get predictions from your models using several images.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-28T08:33:03.028262Z","iopub.status.busy":"2023-08-28T08:33:03.027951Z","iopub.status.idle":"2023-08-28T08:33:15.376356Z","shell.execute_reply":"2023-08-28T08:33:15.375306Z","shell.execute_reply.started":"2023-08-28T08:33:03.028233Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mpizarro/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package punkt to /home/mpizarro/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# Import important libraries\n","import torch\n","import numpy as np\n","import data.preprocessing as pr\n","from torchvision import transforms\n","from transformers import BlipForConditionalGeneration, AutoProcessor"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T08:33:15.378343Z","iopub.status.busy":"2023-08-28T08:33:15.377982Z","iopub.status.idle":"2023-08-28T08:33:15.815212Z","shell.execute_reply":"2023-08-28T08:33:15.813266Z","shell.execute_reply.started":"2023-08-28T08:33:15.378307Z"},"trusted":true},"outputs":[],"source":["batch_size = 2\n","# Get the data\n","uids = np.unique(pr.projections.index)[:300]\n","\n","# Image preprocessing \n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224), antialias=False)\n","])\n","\n","train_data, train_loader, val_data, val_loader, test_data, test_loader = pr.create_dataloaders(uids, pr.IMAGES_PATH, batch_size=batch_size, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T08:33:27.690319Z","iopub.status.busy":"2023-08-28T08:33:27.689979Z","iopub.status.idle":"2023-08-28T08:34:01.738529Z","shell.execute_reply":"2023-08-28T08:34:01.737495Z","shell.execute_reply.started":"2023-08-28T08:33:27.690286Z"},"trusted":true},"outputs":[],"source":["# Load model from Huggingface Transformer library\n","processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","\n","# initialize the optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","model.train()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["for epoch in range(2):\n","    model.train()\n","    for batch in train_loader:\n","        imgs, caps = batch[0], batch[1]\n","        pixel_values, input_ids, _ = processor(images=imgs, text=caps, return_tensors=\"pt\", padding=\"max_length\").values()\n","        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    print(f\"Epoch {epoch} loss: {loss.item()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T09:00:51.363629Z","iopub.status.busy":"2023-08-28T09:00:51.362805Z","iopub.status.idle":"2023-08-28T09:00:52.785622Z","shell.execute_reply":"2023-08-28T09:00:52.784605Z","shell.execute_reply.started":"2023-08-28T09:00:51.363592Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained('blip-model')\n","processor.save_pretrained('blip-processor')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Deploy Model with Gradio:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T09:00:58.743063Z","iopub.status.busy":"2023-08-28T09:00:58.741826Z","iopub.status.idle":"2023-08-28T09:01:19.279518Z","shell.execute_reply":"2023-08-28T09:01:19.278310Z","shell.execute_reply.started":"2023-08-28T09:00:58.743028Z"},"trusted":true},"outputs":[],"source":["# !pip install -q gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T09:01:19.283695Z","iopub.status.busy":"2023-08-28T09:01:19.283370Z","iopub.status.idle":"2023-08-28T09:01:20.981840Z","shell.execute_reply":"2023-08-28T09:01:20.980801Z","shell.execute_reply.started":"2023-08-28T09:01:19.283663Z"},"trusted":true},"outputs":[],"source":["# import gradio as gr\n","# from PIL import Image\n","\n","# processor = AutoProcessor.from_pretrained('blip-processor')\n","# model = BlipForConditionalGeneration.from_pretrained('blip-model')\n","\n","# # Define the prediction function\n","# def generate_caption(image):\n","#     # Process the image\n","#     image = Image.fromarray(image)\n","#     #inputs = tokenizer(image, return_tensors=\"pt\")\n","#     inputs = processor(images=image, return_tensors=\"pt\")#.to(device)\n","#     pixel_values = inputs.pixel_values\n","\n","#     # Generate caption\n","#     generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n","#     generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","#     return generated_caption\n","\n","# # Define the Gradio interface\n","# interface = gr.Interface(\n","#     fn=generate_caption,\n","#     inputs=gr.Image(),\n","#     outputs=gr.Textbox(),\n","#     live=True\n","# )\n","\n","# # Launch the Gradio interface\n","# interface.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
